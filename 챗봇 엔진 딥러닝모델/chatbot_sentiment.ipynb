{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381064bitbaseconda2e0512d94c3c497fb443c803f7bf220f",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "414/414 [==============================] - 24s 55ms/step - loss: 0.9080 - accuracy: 0.5466 - val_loss: 0.6677 - val_accuracy: 0.6569\n",
      "Epoch 2/5\n",
      "414/414 [==============================] - 23s 56ms/step - loss: 0.5442 - accuracy: 0.7815 - val_loss: 0.3027 - val_accuracy: 0.8997\n",
      "Epoch 3/5\n",
      "414/414 [==============================] - 23s 56ms/step - loss: 0.3249 - accuracy: 0.8891 - val_loss: 0.1453 - val_accuracy: 0.9518\n",
      "Epoch 4/5\n",
      "414/414 [==============================] - 23s 56ms/step - loss: 0.2031 - accuracy: 0.9340 - val_loss: 0.0862 - val_accuracy: 0.9746\n",
      "Epoch 5/5\n",
      "414/414 [==============================] - 23s 57ms/step - loss: 0.1298 - accuracy: 0.9621 - val_loss: 0.0733 - val_accuracy: 0.9750\n",
      "60/60 [==============================] - 0s 4ms/step - loss: 0.0841 - accuracy: 0.9763\n",
      "Accuracy: 97.631133\n",
      "loss: 0.084125\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "# 데이터 읽어오기 (Q-질문 데이터, label-감정 라벨링)\n",
    "train_file = \"./chatbot_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "features = data['Q'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터 - 1)토큰으로 쪼개고 2)리스트로 만들고 3)순서마다 번호 부여\n",
    "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "MAX_SEQ_LEN = 15  # 문장마다 다른 단어 시퀀스 벡터 크기를 동일하게 맞춤\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post') #패딩: max_seq_len보다 작은 벡터의 빈 공간에 0을 채움\n",
    "\n",
    "\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성 ➌\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
    "ds = ds.shuffle(len(features))\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(word_index) + 1  # 전체 단어 수\n",
    "\n",
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,)) #입력노드에 들어갈 shape로 시퀀스 벡터크기 지정\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer) #임베딩 계층 만들기; 시퀀스벡터를 희소벡터에서 밀집벡터로 변환\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer) #오버피팅에 대비함 \n",
    "\n",
    "#크기가 각각 3,4,5인 필터 생성, 최대 풀링 연산 수행\n",
    "conv1 = Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "conv2 = Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "conv3 = Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3, 4, 5- gram 이후 합치기: 3개의 Feature map \n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "# 심층신경망. 128개의 출력노드와 relu 활성화함수\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "# 예측할 값이 3개이므로 출력노드는 3개. 예측 최종 단계이므로 활성화함수 X \n",
    "logits = Dense(3, name='logits')(dropout_hidden)\n",
    "# logits에서 나온 점수를 소프트맥스 계층으로 클래스 별 확률 계산  \n",
    "predictions = Dense(3, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss: %f' % (loss))\n",
    "\n",
    "# 모델 저장\n",
    "model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}